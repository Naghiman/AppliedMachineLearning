{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Naghiman/AppliedMachineLearning/blob/master/Multi-Class%20Logistic%20Regression%20and%20Gradient%20Descent/LR_GD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FQ4ZrfbAs63i",
    "outputId": "49c1ea81-4e01-4366-f350-2efd2ee5a6de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# if sklearn version is not up to date then\n",
    "# load_digits(as_frame=True) will fail \n",
    "# run these commands the first time to get version 0.24\n",
    "'''\n",
    "!pip uninstall scikit-learn -y\n",
    "\n",
    "!pip install Cython\n",
    "!pip install git+git://github.com/scikit-learn/scikit-learn.git\n",
    "!pip freeze | grep scikit\n",
    "'''\n",
    "\n",
    "\n",
    "import sklearn\n",
    "sklearn.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT9457uos63p"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jj8xgqDC8dtD"
   },
   "source": [
    "## 1. Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qjd7P36Qs63q",
    "outputId": "334a655e-dfba-43e3-ab0e-67d1062c6d0c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_digits.shape: (1797, 64)\n",
      "y_digits.shape: (1797,)\n"
     ]
    }
   ],
   "source": [
    "# LOAD AND PREPROCESS DIGITS DATASET\n",
    "\n",
    "digits = load_digits(as_frame=True)\n",
    "ddf = digits.frame\n",
    "y_digits = ddf['target'].to_numpy()\n",
    "ddf.drop(\"target\", axis=1, inplace=True)\n",
    "X_digits = ddf.to_numpy()\n",
    "\n",
    "print(f'X_digits.shape: {X_digits.shape}')\n",
    "print(f'y_digits.shape: {y_digits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "InAizZ9D8dtJ"
   },
   "outputs": [],
   "source": [
    "# train, val, test split\n",
    "X_trn_digits, X_tst_digits, y_trn_digits, y_tst_digits = sklearn.model_selection.train_test_split(X_digits, y_digits, test_size=1/5, random_state=0)\n",
    "X_trn_digits, X_val_digits, y_trn_digits, y_val_digits = sklearn.model_selection.train_test_split(X_trn_digits, y_trn_digits, test_size=1/4, random_state=0)\n",
    "\n",
    "# preprocess scales\n",
    "scaler_digits = sklearn.preprocessing.StandardScaler().fit(X_trn_digits)\n",
    "X_trn_digits = scaler_digits.transform(X_trn_digits)\n",
    "X_val_digits = scaler_digits.transform(X_val_digits)\n",
    "X_tst_digits = scaler_digits.transform(X_tst_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SbhoR7f8dtO"
   },
   "source": [
    "## 2. Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QMhqQXus63u",
    "outputId": "e2b2af77-5a33-4b35-da9e-5eb68929333e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_credit.shape: (1000, 61)\n",
      "y_credit.shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# LOAD AND PREPROCESS CREDIT-G DATASET\n",
    "credit = fetch_openml(name='credit-g',as_frame=True)\n",
    "cdf = credit.frame\n",
    "\n",
    "# CONVERT CATEGORICAL FEATURES TO ONE-HOT ENCODING IN CREDIT-G\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df = pd.DataFrame(enc.fit_transform(cdf[['checking_status','credit_history','purpose','savings_status','employment','personal_status','other_parties','property_magnitude','other_payment_plans','housing','job','own_telephone','foreign_worker']]).toarray())\n",
    "cdf = cdf.join(enc_df)\n",
    "cdf.drop(['checking_status','credit_history','purpose','savings_status','employment','personal_status','other_parties','property_magnitude','other_payment_plans','housing','job','own_telephone','foreign_worker'], axis=1, inplace=True)\n",
    "class_dict = {\"bad\": 0, \"good\": 1}\n",
    "y_credit = (cdf.replace({\"class\": class_dict})['class']).to_numpy()\n",
    "cdf.drop(\"class\", axis=1, inplace=True)\n",
    "X_credit = cdf.to_numpy()\n",
    "\n",
    "print(f'X_credit.shape: {X_credit.shape}')\n",
    "print(f'y_credit.shape: {y_credit.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z_l1qFeT8dtV"
   },
   "outputs": [],
   "source": [
    "# train, val, test split\n",
    "X_trn_credit, X_tst_credit, y_trn_credit, y_tst_credit = sklearn.model_selection.train_test_split(X_credit, y_credit, test_size=1/5, random_state=0)\n",
    "X_trn_credit, X_val_credit, y_trn_credit, y_val_credit = sklearn.model_selection.train_test_split(X_trn_credit, y_trn_credit, test_size=1/4, random_state=0)\n",
    "\n",
    "# preprocess scales\n",
    "scaler_credit = sklearn.preprocessing.StandardScaler().fit(X_trn_credit)\n",
    "X_trn_credit = scaler_credit.transform(X_trn_credit)\n",
    "X_val_credit = scaler_credit.transform(X_val_credit)\n",
    "X_tst_credit = scaler_credit.transform(X_tst_credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6zQNzZp8dta"
   },
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyDYhN5GRkHH"
   },
   "source": [
    "## Multi-class logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bJYNhFtY8dtc"
   },
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self, add_bias=True, reg=0):\n",
    "        self.add_bias = add_bias\n",
    "        self.reg = reg\n",
    "\n",
    "    def fit(self, x, y, optimizer):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x, np.ones(N)])\n",
    "        N,D = x.shape\n",
    "        C = len(np.unique(y))\n",
    "\n",
    "        def gradient(x, y, w):                          # define the gradient function\n",
    "            N = x.shape[0]\n",
    "\n",
    "            # Softmax calculation\n",
    "            scores = x.dot(w)\n",
    "            scores -= np.max(scores, axis=1, keepdims=True)\n",
    "            exp_scores = np.exp(scores)\n",
    "            softmax = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "            # dw calculation\n",
    "            indices = np.arange(N)\n",
    "            softmax_editted = softmax\n",
    "            softmax_editted[indices, y] -= 1\n",
    "            dw = np.dot(x.T, softmax_editted)\n",
    "            dw /= N\n",
    "            dw += self.reg * 2 * w\n",
    "            return dw\n",
    "\n",
    "        w0 = np.zeros((D, C))                                # initialize the weights to 0\n",
    "        self.w = optimizer.run(gradient, x, y, w0)      # run the optimizer to get the optimal weights\n",
    "        print(self.w.shape)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.column_stack([x, np.ones(x.shape[0])])\n",
    "        yh = x@self.w\n",
    "        y_pred = np.argmax(yh, axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsGmJKHqRkHH"
   },
   "source": [
    "## Mini-batch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ETd2_GHO8dti"
   },
   "outputs": [],
   "source": [
    "class MiniBatchGradientMomentum:\n",
    "    def __init__(self, learning_rate=.001, batch_size=16, momentum=0.9, max_iters=1e4, epsilon=1e-8, record_history=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.record_history = record_history\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        if record_history:\n",
    "            self.w_history = []                \n",
    "\n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        delta_w = 0\n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            batch_inds = np.random.choice(x.shape[0], self.batch_size)\n",
    "            grad = gradient_fn(x[batch_inds], y[batch_inds], w)\n",
    "            delta_w = self.momentum * delta_w + (1 - self.momentum) * grad              \n",
    "            w = w - self.learning_rate * delta_w       \n",
    "            if self.record_history:\n",
    "                self.w_history.append(w)\n",
    "            t += 1\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvekvffQ8dtn"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk1tVUsG8dto"
   },
   "source": [
    "## 1. Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gL5_hF7K8dtp",
    "outputId": "6e6cb823-1765-40e2-a634-0bdaf2f693bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SoftmaxRegression at 0x2d3823e0888>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = MiniBatchGradientMomentum(learning_rate=.005, max_iters=1000, batch_size=16, record_history=True)\n",
    "model = SoftmaxRegression()\n",
    "model.fit(X_trn_digits, y_trn_digits, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IyjFrjTz8dtt",
    "outputId": "db5987f3-7bb1-4137-ff0c-847f12501d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digits train accuracy: 0.9387186629526463\n",
      "Digits validation accuracy: 0.9083333333333333\n",
      "Digits test accuracy: 0.9222222222222223\n"
     ]
    }
   ],
   "source": [
    "train_acc_digits = sklearn.metrics.accuracy_score(y_trn_digits, model.predict(X_trn_digits))\n",
    "val_acc_digits = sklearn.metrics.accuracy_score(y_val_digits, model.predict(X_val_digits))\n",
    "test_acc_digits = sklearn.metrics.accuracy_score(y_tst_digits, model.predict(X_tst_digits))\n",
    "print(f'Digits train accuracy: {train_acc_digits}')\n",
    "print(f'Digits validation accuracy: {val_acc_digits}')\n",
    "print(f'Digits test accuracy: {test_acc_digits}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSVGfhsZ8dtw"
   },
   "source": [
    "## 2. Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfrMJJ8n8dtx",
    "outputId": "1a5658bf-5d7d-4410-d070-491d1abb8ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SoftmaxRegression at 0x2d38239ce48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = MiniBatchGradientMomentum(learning_rate=.005, max_iters=1000, batch_size=16, record_history=True)\n",
    "model = SoftmaxRegression()\n",
    "model.fit(X_trn_credit, y_trn_credit, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRxV3cVW8dt3",
    "outputId": "b7e35976-f117-4a8b-f01d-bda1d551c20a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit-G train accuracy: 0.8\n",
      "Credit-G validation accuracy: 0.755\n",
      "Credit-G test accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "train_acc_credit = sklearn.metrics.accuracy_score(y_trn_credit, model.predict(X_trn_credit))\n",
    "val_acc_credit = sklearn.metrics.accuracy_score(y_val_credit, model.predict(X_val_credit))\n",
    "test_acc_credit = sklearn.metrics.accuracy_score(y_tst_credit, model.predict(X_tst_credit))\n",
    "print(f'Credit-G train accuracy: {train_acc_credit}')\n",
    "print(f'Credit-G validation accuracy: {val_acc_credit}')\n",
    "print(f'Credit-G test accuracy: {test_acc_credit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imLadUGas64J"
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d8eLdP3ThW03"
   },
   "outputs": [],
   "source": [
    "\n",
    "# softmax class is refactored a bit, predict method moved to optimizer class\n",
    "# validation sets given as parameters to calculate accuracy for each iteration\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, add_bias=True, reg=0):\n",
    "        self.add_bias = add_bias\n",
    "        self.reg = reg\n",
    "\n",
    "    def fit(self, x, y, X_val, y_val, optimizer, limit):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x, np.ones(N)])\n",
    "        N,D = x.shape\n",
    "        C = len(np.unique(y))\n",
    "\n",
    "        def gradient(x, y, w):                          # define the gradient function\n",
    "            N = x.shape[0]\n",
    "\n",
    "            # Softmax calculation\n",
    "            scores = x.dot(w)\n",
    "            scores -= np.max(scores, axis=1, keepdims=True)\n",
    "            exp_scores = np.exp(scores)\n",
    "            softmax = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "            # dw calculation\n",
    "            indices = np.arange(N)\n",
    "            softmax_editted = softmax\n",
    "            softmax_editted[indices, y] -= 1\n",
    "            dw = np.dot(x.T, softmax_editted)\n",
    "            dw /= N\n",
    "            dw += self.reg * 2 * w\n",
    "            return dw\n",
    "\n",
    "        w0 = np.zeros((D, C))                                                # initialize the weights to 0\n",
    "        self.w = optimizer.run(gradient, x, y, w0, X_val, y_val, limit)      # run the optimizer to get the optimal weights      \n",
    "        return self\n",
    "\n",
    "\n",
    "class MiniBatchGradientMomentum:\n",
    "    def __init__(self, add_bias=True, learning_rate=.001, batch_size=16, momentum=0.9, max_iters=25000, epsilon=1e-8, record_history=False):\n",
    "        self.add_bias = add_bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.record_history = record_history\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        if record_history:\n",
    "            self.w_history = []                \n",
    "\n",
    "    def run(self, gradient_fn, x, y, w, X_val, y_val, limit):\n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        delta_w = 0\n",
    "        val_accs = []\n",
    "\n",
    "        # need at least (limit) iterations to look at\n",
    "        # check validation accuracy values have not decreased for last (limit) iterations\n",
    "        # add a max iterations check so we don't loop forever in case 2 conditions above fail to stop loop\n",
    "        while t < self.max_iters and (t < limit or not self.decreasing(val_accs, t, limit, self.epsilon)):\n",
    "            batch_inds = np.random.choice(x.shape[0], self.batch_size)\n",
    "            grad = gradient_fn(x[batch_inds], y[batch_inds], w)\n",
    "            delta_w = self.momentum * delta_w + (1 - self.momentum) * grad              \n",
    "            w = w - self.learning_rate * delta_w       \n",
    "            if self.record_history:\n",
    "                self.w_history.append(w)\n",
    "            val_acc = sklearn.metrics.accuracy_score(y_val, self.predict(X_val, w))\n",
    "            val_accs.append(val_acc)\n",
    "            t += 1\n",
    "      \n",
    "        # it is possible the last (limit) iterations happened at the exact so we need an extra check\n",
    "        # for non decreasing values\n",
    "        if t == self.max_iters and not self.decreasing(val_accs, t-1, limit, self.epsilon):\n",
    "            print(f'best validation accuracy ({val_acc}) overshoot to the maximum {t} iterations based on last {limit} values')\n",
    "        else:\n",
    "            print(f'best validation accuracy ({val_acc}) found after {t} iterations based on last {limit} values')\n",
    "\n",
    "        # since we stored accuracy values at each iteration we can compare with the real best value\n",
    "        print(f'true best result is {max(val_accs)} which happened after {val_accs.index(max(val_accs))} iterations')\n",
    "        return w\n",
    "    \n",
    "    def decreasing(self, values, t, limit, epsilon):\n",
    "        return all(0<=x-y<=epsilon for x, y in zip(values[t-limit:t], values[t-limit+1:t+1]))\n",
    "\n",
    "    def predict(self, x, w):\n",
    "        if self.add_bias:\n",
    "            x = np.column_stack([x, np.ones(x.shape[0])])\n",
    "        yh = x@w\n",
    "        y_pred = np.argmax(yh, axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDsD0iV2jvJt",
    "outputId": "3336ad8f-b6c0-4e98-f8ed-43dc4f442582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy (0.765) found after 910 iterations based on last 200 values\n",
      "true best result is 0.815 which happened after 21 iterations\n"
     ]
    }
   ],
   "source": [
    "model = SoftmaxRegression()\n",
    "\n",
    "optimizer = MiniBatchGradientMomentum(learning_rate=.005, max_iters=10000, batch_size=16, record_history=True)\n",
    "model.fit(X_trn_credit, y_trn_credit, X_val_credit, y_val_credit, optimizer, 200)\n",
    "\n",
    "optimizer = MiniBatchGradientMomentum(learning_rate=.005, max_iters=20000, batch_size=16, record_history=True)\n",
    "model.fit(X_trn_digits, y_trn_digits, X_val_digits, y_val_digits, optimizer, 4000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "COVIDSTH_Revised.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
