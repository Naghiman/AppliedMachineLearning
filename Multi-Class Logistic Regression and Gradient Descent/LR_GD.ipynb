{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVIDSTH_Revised.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naghiman/AppliedMachineLearning/blob/master/Multi-Class%20Logistic%20Regression%20and%20Gradient%20Descent/LR_GD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ4ZrfbAs63i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49c1ea81-4e01-4366-f350-2efd2ee5a6de"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# if sklearn version is not up to date then\n",
        "# load_digits(as_frame=True) will fail \n",
        "# run these commands the first time to get version 0.24\n",
        "'''\n",
        "!pip uninstall scikit-learn -y\n",
        "\n",
        "!pip install Cython\n",
        "!pip install git+git://github.com/scikit-learn/scikit-learn.git\n",
        "!pip freeze | grep scikit\n",
        "'''\n",
        "\n",
        "\n",
        "import sklearn\n",
        "sklearn.__version__\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.24.dev0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT9457uos63p"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj8xgqDC8dtD"
      },
      "source": [
        "## 1. Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjd7P36Qs63q",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334a655e-dfba-43e3-ab0e-67d1062c6d0c"
      },
      "source": [
        "# LOAD AND PREPROCESS DIGITS DATASET\n",
        "\n",
        "digits = load_digits(as_frame=True)\n",
        "ddf = digits.frame\n",
        "y_digits = ddf['target'].to_numpy()\n",
        "ddf.drop(\"target\", axis=1, inplace=True)\n",
        "X_digits = ddf.to_numpy()\n",
        "\n",
        "print(f'X_digits.shape: {X_digits.shape}')\n",
        "print(f'y_digits.shape: {y_digits.shape}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_digits.shape: (1797, 64)\n",
            "y_digits.shape: (1797,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InAizZ9D8dtJ"
      },
      "source": [
        "# train, val, test split\n",
        "X_trn_digits, X_tst_digits, y_trn_digits, y_tst_digits = sklearn.model_selection.train_test_split(X_digits, y_digits, test_size=1/5, random_state=0)\n",
        "X_trn_digits, X_val_digits, y_trn_digits, y_val_digits = sklearn.model_selection.train_test_split(X_trn_digits, y_trn_digits, test_size=1/4, random_state=0)\n",
        "\n",
        "# preprocess scales\n",
        "scaler_digits = sklearn.preprocessing.StandardScaler().fit(X_trn_digits)\n",
        "X_trn_digits = scaler_digits.transform(X_trn_digits)\n",
        "X_val_digits = scaler_digits.transform(X_val_digits)\n",
        "X_tst_digits = scaler_digits.transform(X_tst_digits)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SbhoR7f8dtO"
      },
      "source": [
        "## 2. Credit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QMhqQXus63u",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b2af77-5a33-4b35-da9e-5eb68929333e"
      },
      "source": [
        "# LOAD AND PREPROCESS CREDIT-G DATASET\n",
        "credit = fetch_openml(name='credit-g',as_frame=True)\n",
        "cdf = credit.frame\n",
        "\n",
        "# CONVERT CATEGORICAL FEATURES TO ONE-HOT ENCODING IN CREDIT-G\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "enc_df = pd.DataFrame(enc.fit_transform(cdf[['checking_status','credit_history','purpose','savings_status','employment','personal_status','other_parties','property_magnitude','other_payment_plans','housing','job','own_telephone','foreign_worker']]).toarray())\n",
        "cdf = cdf.join(enc_df)\n",
        "cdf.drop(['checking_status','credit_history','purpose','savings_status','employment','personal_status','other_parties','property_magnitude','other_payment_plans','housing','job','own_telephone','foreign_worker'], axis=1, inplace=True)\n",
        "class_dict = {\"bad\": 0, \"good\": 1}\n",
        "y_credit = (cdf.replace({\"class\": class_dict})['class']).to_numpy()\n",
        "cdf.drop(\"class\", axis=1, inplace=True)\n",
        "X_credit = cdf.to_numpy()\n",
        "\n",
        "print(f'X_credit.shape: {X_credit.shape}')\n",
        "print(f'y_credit.shape: {y_credit.shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_credit.shape: (1000, 61)\n",
            "y_credit.shape: (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_l1qFeT8dtV"
      },
      "source": [
        "# train, val, test split\n",
        "X_trn_credit, X_tst_credit, y_trn_credit, y_tst_credit = sklearn.model_selection.train_test_split(X_credit, y_credit, test_size=1/5, random_state=0)\n",
        "X_trn_credit, X_val_credit, y_trn_credit, y_val_credit = sklearn.model_selection.train_test_split(X_trn_credit, y_trn_credit, test_size=1/4, random_state=0)\n",
        "\n",
        "# preprocess scales\n",
        "scaler_credit = sklearn.preprocessing.StandardScaler().fit(X_trn_credit)\n",
        "X_trn_credit = scaler_credit.transform(X_trn_credit)\n",
        "X_val_credit = scaler_credit.transform(X_val_credit)\n",
        "X_tst_credit = scaler_credit.transform(X_tst_credit)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6zQNzZp8dta"
      },
      "source": [
        "# Softmax Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyDYhN5GRkHH"
      },
      "source": [
        "## Multi-class logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJYNhFtY8dtc"
      },
      "source": [
        "class SoftmaxRegression:\n",
        "    def __init__(self, add_bias=True, reg=0):\n",
        "        self.add_bias = add_bias\n",
        "        self.reg = reg\n",
        "\n",
        "    def fit(self, x, y, optimizer):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x, np.ones(N)])\n",
        "        N,D = x.shape\n",
        "        C = len(np.unique(y))\n",
        "\n",
        "        def gradient(x, y, w):                          # define the gradient function\n",
        "            N = x.shape[0]\n",
        "\n",
        "            # Softmax calculation\n",
        "            scores = x.dot(w)\n",
        "            scores -= np.max(scores, axis=1, keepdims=True)\n",
        "            exp_scores = np.exp(scores)\n",
        "            softmax = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "            # dw calculation\n",
        "            indices = np.arange(N)\n",
        "            softmax_editted = softmax\n",
        "            softmax_editted[indices, y] -= 1\n",
        "            dw = np.dot(x.T, softmax_editted)\n",
        "            dw /= N\n",
        "            dw += self.reg * 2 * w\n",
        "            return dw\n",
        "\n",
        "        w0 = np.zeros((D, C))                                # initialize the weights to 0\n",
        "        self.w = optimizer.run(gradient, x, y, w0)      # run the optimizer to get the optimal weights\n",
        "        print(self.w.shape)\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x):\n",
        "        if self.add_bias:\n",
        "            x = np.column_stack([x, np.ones(x.shape[0])])\n",
        "        yh = x@self.w\n",
        "        y_pred = np.argmax(yh, axis=1)\n",
        "        return y_pred"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsGmJKHqRkHH"
      },
      "source": [
        "## Mini-batch optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETd2_GHO8dti"
      },
      "source": [
        "class MiniBatchGradientMomentum:\n",
        "    def __init__(self, learning_rate=.001, batch_size=16, momentum=0.9, max_iters=1e4, epsilon=1e-8, record_history=False):\n",
        "      self.learning_rate = learning_rate\n",
        "      self.max_iters = max_iters\n",
        "      self.record_history = record_history\n",
        "      self.epsilon = epsilon\n",
        "      self.momentum = momentum\n",
        "      self.batch_size = batch_size\n",
        "      if record_history:\n",
        "          self.w_history = []                \n",
        "\n",
        "    def run(self, gradient_fn, x, y, w):\n",
        "      grad = np.inf\n",
        "      t = 1\n",
        "      delta_w = 0\n",
        "      while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
        "          batch_inds = np.random.choice(x.shape[0], self.batch_size)\n",
        "          grad = gradient_fn(x[batch_inds], y[batch_inds], w)\n",
        "          delta_w = self.momentum * delta_w + (1 - self.momentum) * grad              \n",
        "          w = w - self.learning_rate * delta_w       \n",
        "          if self.record_history:\n",
        "              self.w_history.append(w)\n",
        "          t += 1\n",
        "      return w"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvekvffQ8dtn"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk1tVUsG8dto"
      },
      "source": [
        "## 1. Digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL5_hF7K8dtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e6cb823-1765-40e2-a634-0bdaf2f693bf"
      },
      "source": [
        "optimizer = MiniBatchGradientMomentum(learning_rate=.005, max_iters=1000, batch_size=16, record_history=True)\n",
        "model = SoftmaxRegression()\n",
        "model.fit(X_trn_digits, y_trn_digits, optimizer)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(65, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.SoftmaxRegression at 0x7f15eb854978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyjFrjTz8dtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5987f3-7bb1-4137-ff0c-847f12501d55"
      },
      "source": [
        "train_acc_digits = sklearn.metrics.accuracy_score(y_trn_digits, model.predict(X_trn_digits))\n",
        "val_acc_digits = sklearn.metrics.accuracy_score(y_val_digits, model.predict(X_val_digits))\n",
        "test_acc_digits = sklearn.metrics.accuracy_score(y_tst_digits, model.predict(X_tst_digits))\n",
        "print(f'Digits train accuracy: {train_acc_digits}')\n",
        "print(f'Digits validation accuracy: {val_acc_digits}')\n",
        "print(f'Digits test accuracy: {test_acc_digits}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Digits train accuracy: 0.9368616527390901\n",
            "Digits validation accuracy: 0.8972222222222223\n",
            "Digits test accuracy: 0.9166666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSVGfhsZ8dtw"
      },
      "source": [
        "## 2. Credit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfrMJJ8n8dtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5658bf-5d7d-4410-d070-491d1abb8ad4"
      },
      "source": [
        "optimizer = MiniBatchGradientMomentum(learning_rate=.005, max_iters=1000, batch_size=16, record_history=True)\n",
        "model = SoftmaxRegression()\n",
        "model.fit(X_trn_credit, y_trn_credit, optimizer)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(62, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.SoftmaxRegression at 0x7f15eb854b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRxV3cVW8dt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7e35976-f117-4a8b-f01d-bda1d551c20a"
      },
      "source": [
        "train_acc_credit = sklearn.metrics.accuracy_score(y_trn_credit, model.predict(X_trn_credit))\n",
        "val_acc_credit = sklearn.metrics.accuracy_score(y_val_credit, model.predict(X_val_credit))\n",
        "test_acc_credit = sklearn.metrics.accuracy_score(y_tst_credit, model.predict(X_tst_credit))\n",
        "print(f'Credit-G train accuracy: {train_acc_credit}')\n",
        "print(f'Credit-G validation accuracy: {val_acc_credit}')\n",
        "print(f'Credit-G test accuracy: {test_acc_credit}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Credit-G train accuracy: 0.795\n",
            "Credit-G validation accuracy: 0.76\n",
            "Credit-G test accuracy: 0.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imLadUGas64J"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8eLdP3ThW03"
      },
      "source": [
        "\n",
        "# softmax class is refactored a bit, predict method moved to optimizer class\n",
        "# validation sets given as parameters to calculate accuracy for each iteration\n",
        "class SoftmaxRegression:\n",
        "    def __init__(self, add_bias=True, reg=0):\n",
        "        self.add_bias = add_bias\n",
        "        self.reg = reg\n",
        "\n",
        "    def fit(self, x, y, X_val, y_val, optimizer, limit):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x, np.ones(N)])\n",
        "        N,D = x.shape\n",
        "        C = len(np.unique(y))\n",
        "\n",
        "        def gradient(x, y, w):                          # define the gradient function\n",
        "            N = x.shape[0]\n",
        "\n",
        "            # Softmax calculation\n",
        "            scores = x.dot(w)\n",
        "            scores -= np.max(scores, axis=1, keepdims=True)\n",
        "            exp_scores = np.exp(scores)\n",
        "            softmax = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "            # dw calculation\n",
        "            indices = np.arange(N)\n",
        "            softmax_editted = softmax\n",
        "            softmax_editted[indices, y] -= 1\n",
        "            dw = np.dot(x.T, softmax_editted)\n",
        "            dw /= N\n",
        "            dw += self.reg * 2 * w\n",
        "            return dw\n",
        "\n",
        "        w0 = np.zeros((D, C))                                                # initialize the weights to 0\n",
        "        self.w = optimizer.run(gradient, x, y, w0, X_val, y_val, limit)      # run the optimizer to get the optimal weights      \n",
        "        return self\n",
        "\n",
        "\n",
        "class MiniBatchGradientMomentum:\n",
        "    def __init__(self, add_bias=True, learning_rate=.001, batch_size=16, momentum=0.9, max_iters=25000, epsilon=1e-8, record_history=False):\n",
        "      self.add_bias = add_bias\n",
        "      self.learning_rate = learning_rate\n",
        "      self.max_iters = max_iters\n",
        "      self.record_history = record_history\n",
        "      self.epsilon = epsilon\n",
        "      self.momentum = momentum\n",
        "      self.batch_size = batch_size\n",
        "      if record_history:\n",
        "          self.w_history = []                \n",
        "\n",
        "    def run(self, gradient_fn, x, y, w, X_val, y_val, limit):\n",
        "      grad = np.inf\n",
        "      t = 1\n",
        "      delta_w = 0\n",
        "      val_accs = []\n",
        "\n",
        "      # need at least (limit) iterations to look at\n",
        "      # check validation accuracy values have not decreased for last (limit) iterations\n",
        "      # add a max iterations check so we don't loop forever in case 2 conditions above fail to stop loop\n",
        "      while t < self.max_iters and (t < limit or not self.decreasing(val_accs, t, limit, self.epsilon)):\n",
        "          batch_inds = np.random.choice(x.shape[0], self.batch_size)\n",
        "          grad = gradient_fn(x[batch_inds], y[batch_inds], w)\n",
        "          delta_w = self.momentum * delta_w + (1 - self.momentum) * grad              \n",
        "          w = w - self.learning_rate * delta_w       \n",
        "          if self.record_history:\n",
        "              self.w_history.append(w)\n",
        "          val_acc = sklearn.metrics.accuracy_score(y_val, self.predict(X_val, w))\n",
        "          val_accs.append(val_acc)\n",
        "          t += 1\n",
        "      \n",
        "      # it is possible the last (limit) iterations happened at the exact so we need an extra check\n",
        "      # for non decreasing values\n",
        "      if t == self.max_iters and not self.decreasing(val_accs, t-1, limit, self.epsilon):\n",
        "        print(f'best validation accuracy ({val_acc}) overshoot to the maximum {t} iterations based on last {limit} values')\n",
        "      else:\n",
        "        print(f'best validation accuracy ({val_acc}) found after {t} iterations based on last {limit} values')\n",
        "\n",
        "      # since we stored accuracy values at each iteration we can compare with the real best value\n",
        "      print(f'true best result is {max(val_accs)} which happened after {val_accs.index(max(val_accs))} iterations')\n",
        "      return w\n",
        "    \n",
        "    def decreasing(self, values, t, limit, epsilon):\n",
        "      return all(0<=x-y<=epsilon for x, y in zip(values[t-limit:t], values[t-limit+1:t+1]))\n",
        "\n",
        "    def predict(self, x, w):\n",
        "      if self.add_bias:\n",
        "          x = np.column_stack([x, np.ones(x.shape[0])])\n",
        "      yh = x@w\n",
        "      y_pred = np.argmax(yh, axis=1)\n",
        "      return y_pred"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDsD0iV2jvJt",
        "outputId": "3336ad8f-b6c0-4e98-f8ed-43dc4f442582",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SoftmaxRegression()\n",
        "\n",
        "optimizer = MiniBatchGradientMomentum(learning_rate=.005, max_iters=10000, batch_size=16, record_history=True)\n",
        "model.fit(X_trn_credit, y_trn_credit, X_val_credit, y_val_credit, optimizer, 200)\n",
        "\n",
        "optimizer = MiniBatchGradientMomentum(learning_rate=.005, max_iters=20000, batch_size=16, record_history=True)\n",
        "model.fit(X_trn_digits, y_trn_digits, X_val_digits, y_val_digits, optimizer, 4000)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best validation accuracy (0.77) overshoot to the maximum 10000 iterations based on last 200 values\n",
            "true best result is 0.785 which happened after 4082 iterations\n",
            "best validation accuracy (0.9611111111111111) overshoot to the maximum 20000 iterations based on last 4000 values\n",
            "true best result is 0.9638888888888889 which happened after 15282 iterations\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.SoftmaxRegression at 0x7f15eb8532b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}