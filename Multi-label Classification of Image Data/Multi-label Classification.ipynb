{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\\ Import libraries and dataset\n"
    }
   },
   "outputs": [],
   "source": [
    "import h5py as h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# ! wget https://drive.google.com/file/d/1LcKqf1d7bctw5lx0YZf31kCUF0zEYOsi/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: torch.Size([56000, 1, 64, 64]), dtype torch.uint8\n",
      "y_train.shape: torch.Size([56000, 5]), dtype torch.int32\n",
      "X_test.shape: torch.Size([14000, 1, 64, 64]), dtype torch.uint8\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('MNIST_synthetic.h5', 'r')\n",
    "\n",
    "X_train = torch.tensor(f['train_dataset'][...]).permute(0, 3, 1, 2).contiguous()\n",
    "y_train = torch.tensor(f['train_labels'][...])\n",
    "X_test = torch.tensor(f['test_dataset'][...]).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "print(f'X_train.shape: {X_train.shape}, dtype {X_train.dtype}')\n",
    "print(f'y_train.shape: {y_train.shape}, dtype {y_train.dtype}')\n",
    "print(f'X_test.shape: {X_test.shape}, dtype {X_test.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scale(tensor):\n",
    "    \"\"\"\n",
    "    [0, 255] -> [-1, 1]\n",
    "    \"\"\"\n",
    "    return (tensor.to(torch.float32) * 2 / 255) - 1\n",
    "\n",
    "def descale(tensor):\n",
    "    \"\"\"\n",
    "    [-1, 1] -> [0, 255]\n",
    "    \"\"\"\n",
    "    return ((tensor + 1) * 255 / 2).to(torch.uint8)\n",
    "\n",
    "def visualize_tensor(tensor):\n",
    "    \"\"\"\n",
    "    visualize a tensor of shape (H, W, 1). Must be in range [-1, 1] or [0, 255].\n",
    "    \"\"\"\n",
    "    if tensor.dtype == torch.float32:\n",
    "        tensor = descale(tensor)\n",
    "    array = tensor.permute(1, 2, 0).contiguous().numpy()\n",
    "    plt.imshow(array, cmap='Greys_r')\n",
    "    plt.show()\n",
    "\n",
    "# Class to represent our dataset and be used with dataloader\n",
    "class CustomMNISTDataset(TensorDataset):\n",
    "    def __init__(self):\n",
    "        self.len = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "dataset = CustomMNISTDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_size = 12  # 64 // 5\n",
    "crop_h_start = (64 // 2) - (digit_size // 2)\n",
    "crop_h_end = crop_h_start + digit_size +1  # +1 is safety margin\n",
    "crop_w_start = 2\n",
    "crop_w_end = -2\n",
    "\n",
    "def split_digits(tensor, num_digits, digit_size=digit_size):\n",
    "    \"\"\"\n",
    "    splits a single image of `num_digits` digits to multiple images\n",
    "    \"\"\"\n",
    "    start_index = (5 - num_digits) * digit_size // 2\n",
    "    end_index = start_index + (num_digits * digit_size)\n",
    "    tensor = tensor[..., start_index: end_index]\n",
    "    return torch.stack(torch.chunk(tensor, num_digits, dim=2))\n",
    "\n",
    "# remove padding and rescale\n",
    "X_train_processed = scale(X_train[..., crop_h_start: crop_h_end, crop_w_start: crop_w_end])\n",
    "y_train_processed = y_train.to(torch.int64)\n",
    "X_test_processed = scale(X_test[..., crop_h_start: crop_h_end, crop_w_start: crop_w_end])\n",
    "\n",
    "# TODO: Create validation set randomly (with a fixed random seed) and val_loader\n",
    "num_digits_train_dataset = None\n",
    "num_digits_train_loader = None\n",
    "num_digits_val_dataset = None\n",
    "num_digits_val_loader = None\n",
    "\"\"\"\n",
    "dataset = TensorDataset(X_train_processed, y_train_processed)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\"\"\"\n",
    "\n",
    "# TODO: create a new dataset and loader from the previous dataset by splitting all digits and stacking them all together\n",
    "# such that the new dataset is of shape (number of total digits x 1 x 28 x 28)\n",
    "digits_train_dataset = None\n",
    "digits_train_loader = None\n",
    "digits_val_dataset = None\n",
    "digits_val_loader = None\n",
    "\n",
    "test_loader = DataLoader(X_test_processed, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw X_train\n",
      "------\n",
      "\n",
      "Preprocessed X_train\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 8004, 14324, 10140, 10524, 13964, 4660) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mEmpty\u001B[0m                                     Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    871\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 872\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_data_queue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    873\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\multiprocessing\\queues.py\u001B[0m in \u001B[0;36mget\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    107\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_poll\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m                         \u001B[1;32mraise\u001B[0m \u001B[0mEmpty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m                 \u001B[1;32melif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_poll\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mEmpty\u001B[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-42-0c0ae5ede46b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'------\\n'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Preprocessed X_train'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mx_sample\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_sample\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'Label: {y_sample[0]}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mvisualize_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_sample\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    433\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    434\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 435\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    436\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    437\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1066\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1067\u001B[0m             \u001B[1;32massert\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_shutdown\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tasks_outstanding\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1068\u001B[1;33m             \u001B[0midx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1069\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tasks_outstanding\u001B[0m \u001B[1;33m-=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1070\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1032\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1033\u001B[0m             \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1034\u001B[1;33m                 \u001B[0msuccess\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_try_get_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1035\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0msuccess\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1036\u001B[0m                     \u001B[1;32mreturn\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    883\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfailed_workers\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    884\u001B[0m                 \u001B[0mpids_str\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m', '\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpid\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfailed_workers\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 885\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpids_str\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    886\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mqueue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmpty\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    887\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 8004, 14324, 10140, 10524, 13964, 4660) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "print('Raw X_train')\n",
    "visualize_tensor(X_train[102])\n",
    "print('------\\n')\n",
    "print('Preprocessed X_train')\n",
    "x_sample, y_sample = next(iter(train_loader))\n",
    "print(f'Label: {y_sample[0]}')\n",
    "visualize_tensor(x_sample[0])\n",
    "print('Splitted')\n",
    "num_digits = torch.sum(y_sample[0] != 10).item()\n",
    "splitted_digits = split_digits(x_sample[0], num_digits)\n",
    "for digit in splitted_digits:\n",
    "    visualize_tensor(digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convolutional neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: STEP 1\n",
    "# Train a NN to predict num_digits in each sample. I.e. input would be X from num_digits_train_dataset, output would be\n",
    "# torch.sum(y[0] != 10).item()\n",
    "# This model should have accuracy very close to 100% (>99% on validation)\n",
    "# IF IT DOESN'T, MIGHT NEED DATA AUGMENTATION HERE\n",
    "# Track validation accuracy during training to avoid overfitting (stop training when\n",
    "# validation accuracy starts to decrease)\n",
    "# FANCY TIPS: Use batchnorm layers, lookup VGG/RESNET architecture\n",
    "\n",
    "# TODO: STEP 2\n",
    "# Train a NN to predict each 12x12 digit in digits_train_dataset\n",
    "# Track validation accuracy during training to avoid overfitting (stop training when\n",
    "# validation accuracy starts to decrease)\n",
    "# FANCY TIPS: Use batchnorm layers, lookup VGG/RESNET architecture\n",
    "\n",
    "# TODO: STEP 3\n",
    "# Make num_digits prediction for X_test based on model in STEP1, then use `split_digits()` function \n",
    "# with the predicted number of digits to split the digits, then make a prediction for each digit\n",
    "# separately by using the model trained in STEP 2. Convert the predictions from e.g. 2, 3\n",
    "# into the required format e.g. [2, 3, 10, 10, 10]\n",
    "\n",
    "# TODO: STEP 4\n",
    "# Convert predictions into whatever Excel format is required for uploading to Kaggle\n",
    "\n",
    "# TODO: EXTRA IMPROVEMENTS\n",
    "# - Data Augmentation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # Ensure model weights initialized with same random numbers\n",
    "\n",
    "# # model 0\n",
    "# num_filters = 8\n",
    "# filter_size = 5\n",
    "# pool_size = 4\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Conv2d(in_channels=1,\n",
    "#                     out_channels=num_filters,\n",
    "#                     kernel_size=filter_size,\n",
    "#                     padding=filter_size // 2),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "#     torch.nn.Flatten(),\n",
    "#     torch.nn.Linear(num_filters * 64**2 // pool_size**2, 10),\n",
    "# )\n",
    "\n",
    "\n",
    "# model 1\n",
    "num_filters = [6, 16]\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "mlp_size = [4096, 120, 84, 5 * 11]\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=1,\n",
    "                    out_channels=num_filters[0],\n",
    "                    kernel_size=filter_size,\n",
    "                    padding=filter_size // 2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "    torch.nn.Conv2d(in_channels=num_filters[0],\n",
    "                    out_channels=num_filters[1],\n",
    "                    kernel_size=filter_size,\n",
    "                    padding=filter_size // 2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(mlp_size[0], mlp_size[1]),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(mlp_size[1], mlp_size[2]),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(mlp_size[2], mlp_size[3]),\n",
    ")\n",
    "\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss function and Optimizer\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-43-e06e27e19a9c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;31m# GD is terrible, uses too much memory\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;31m# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[0moptimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# converges faster and less sensitive to lr, generalizes (typiaclly) worse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# cross entropy is pretty good for classification problems\n",
    "# discuss pros of cross entropy in the report\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# might need to play a bit with learning rate and momentum if needed\n",
    "# SGD is computationally efficient\n",
    "# GD is terrible, uses too much memory\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters())  # converges faster and less sensitive to lr, generalizes (typiaclly) worse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training CNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 7636, 14476, 11880, 3212, 6416, 4188) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mEmpty\u001B[0m                                     Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    871\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 872\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_data_queue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    873\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\queue.py\u001B[0m in \u001B[0;36mget\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    177\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[0mremaining\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0.0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 178\u001B[1;33m                         \u001B[1;32mraise\u001B[0m \u001B[0mEmpty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    179\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnot_empty\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mremaining\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mEmpty\u001B[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-48-19c443566dea>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mrunning_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;31m# counter\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;31m# get inputs, data is a list of [inputs, labels]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m         \u001B[1;31m# zero the parameter gradients\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    433\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    434\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 435\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    436\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    437\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1066\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1067\u001B[0m             \u001B[1;32massert\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_shutdown\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tasks_outstanding\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1068\u001B[1;33m             \u001B[0midx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1069\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tasks_outstanding\u001B[0m \u001B[1;33m-=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1070\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1022\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1023\u001B[0m             \u001B[1;32mwhile\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory_thread\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_alive\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1024\u001B[1;33m                 \u001B[0msuccess\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_try_get_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1025\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0msuccess\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1026\u001B[0m                     \u001B[1;32mreturn\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    883\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfailed_workers\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    884\u001B[0m                 \u001B[0mpids_str\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m', '\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpid\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfailed_workers\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 885\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpids_str\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    886\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mqueue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEmpty\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    887\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 7636, 14476, 11880, 3212, 6416, 4188) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# loop over dataset multiple times\n",
    "# model.train()\n",
    "for epoch in range(2):\n",
    "    running_loss = 0 # counter\n",
    "    # get inputs, data is a list of [inputs, labels]\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss = 0\n",
    "        for j in range(5):\n",
    "            loss += criterion(outputs[:, (j * 11):((j + 1) * 11)], labels[:, j])\n",
    "\n",
    "        loss.backward() # computing gradients wrt model's weights\n",
    "        optimizer.step() # update using learning rate\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # print every 2000 minibatches\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss/2000))\n",
    "            running_loss = 0\n",
    "\n",
    "print('Done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_input = next(iter(test_loader))\n",
    "test_output = model(test_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def output_to_label(output):\n",
    "    labels = [torch.argmax(output[:, (j * 11):((j + 1) * 11)], dim=1) for j in range(5)]\n",
    "    return torch.stack(labels)\n",
    "test_pred_labels = output_to_label(test_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_pred_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # Ensure model weights initialized with same random numbers\n",
    "\n",
    "# # model 0\n",
    "# num_filters = 8\n",
    "# filter_size = 5\n",
    "# pool_size = 4\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Conv2d(in_channels=1,\n",
    "#                     out_channels=num_filters,\n",
    "#                     kernel_size=filter_size,\n",
    "#                     padding=filter_size // 2),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "#     torch.nn.Flatten(),\n",
    "#     torch.nn.Linear(num_filters * 64**2 // pool_size**2, 10),\n",
    "# )\n",
    "\n",
    "\n",
    "# model 1\n",
    "num_filters = [6, 16]\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "mlp_size = [4096, 120, 84, 5 * 11]\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=1,\n",
    "                    out_channels=num_filters[0],\n",
    "                    kernel_size=filter_size,\n",
    "                    padding=filter_size // 2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "    torch.nn.Conv2d(in_channels=num_filters[0],\n",
    "                    out_channels=num_filters[1],\n",
    "                    kernel_size=filter_size,\n",
    "                    padding=filter_size // 2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(mlp_size[0], mlp_size[1]),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(mlp_size[1], mlp_size[2]),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(mlp_size[2], mlp_size[3]),\n",
    ")\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loss function and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy is pretty good for classification problems\n",
    "# discuss pros of cross entropy in the report\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# might need to play a bit with learning rate and momentum if needed\n",
    "# SGD is computationally efficient\n",
    "# GD is terrible, uses too much memory\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters())  # converges faster and less sensitive to lr, generalizes (typiaclly) worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-51-19c443566dea>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[1;31m# zero the parameter gradients\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[1;31m# forward + backward + optimize\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "# loop over dataset multiple times\n",
    "# model.train()\n",
    "for epoch in range(2):\n",
    "    running_loss = 0 # counter\n",
    "    # get inputs, data is a list of [inputs, labels]\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss = 0\n",
    "        for j in range(5):\n",
    "            loss += criterion(outputs[:, (j * 11):((j + 1) * 11)], labels[:, j])\n",
    "\n",
    "        loss.backward() # computing gradients wrt model's weights\n",
    "        optimizer.step() # update using learning rate\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # print every 2000 minibatches\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss/2000))\n",
    "            running_loss = 0\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_input = next(iter(test_loader))\n",
    "test_output = model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_label(output):\n",
    "    labels = [torch.argmax(output[:, (j * 11):((j + 1) * 11)], dim=1) for j in range(5)]\n",
    "    return torch.stack(labels)\n",
    "test_pred_labels = output_to_label(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}