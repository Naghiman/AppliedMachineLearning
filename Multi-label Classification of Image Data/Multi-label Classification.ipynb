{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\\ Import libraries and dataset\n"
    }
   },
   "outputs": [],
   "source": [
    "import h5py as h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# ! wget https://drive.google.com/file/d/1LcKqf1d7bctw5lx0YZf31kCUF0zEYOsi/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: torch.Size([56000, 1, 64, 64]), dtype torch.uint8\n",
      "y_train.shape: torch.Size([56000, 5]), dtype torch.int32\n",
      "X_test.shape: torch.Size([14000, 1, 64, 64]), dtype torch.uint8\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('MNIST_synthetic.h5', 'r')\n",
    "\n",
    "X_train = torch.tensor(f['train_dataset'][...]).permute(0, 3, 1, 2).contiguous()\n",
    "y_train = torch.tensor(f['train_labels'][...])\n",
    "X_test = torch.tensor(f['test_dataset'][...]).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "# These are the tensors for our 2nd CNN which predicts the number of digits in an image\n",
    "X_train2 = torch.tensor(f['train_dataset'][...])\n",
    "y_train2 = torch.tensor(f['train_labels'][...])\n",
    "X_test2 = torch.tensor(f['test_dataset'][...])\n",
    "\n",
    "\n",
    "# can refactor to map lambda, same performance\n",
    "def count_digits(tensor):\n",
    "    counter = 0\n",
    "    for i,t in enumerate(tensor):\n",
    "        if tensor[i] != 10:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def count_num_digits(labels):\n",
    "    new_labels = torch.empty(len(y_train2), 1 , dtype=torch.int32)\n",
    "    for i,t in enumerate(labels):\n",
    "        new_labels[i] = count_digits(t)\n",
    "    return new_labels\n",
    "\n",
    "num_of_digits = count_num_digits(y_train2)\n",
    "\n",
    "\n",
    "print(f'X_train.shape: {X_train.shape}, dtype {X_train.dtype}')\n",
    "print(f'y_train.shape: {y_train.shape}, dtype {y_train.dtype}')\n",
    "print(f'X_test.shape: {X_test.shape}, dtype {X_test.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scale(tensor):\n",
    "    \"\"\"\n",
    "    [0, 255] -> [-1, 1]\n",
    "    \"\"\"\n",
    "    return (tensor.to(torch.float32) * 2 / 255) - 1\n",
    "\n",
    "def descale(tensor):\n",
    "    \"\"\"\n",
    "    [-1, 1] -> [0, 255]\n",
    "    \"\"\"\n",
    "    return ((tensor + 1) * 255 / 2).to(torch.uint8)\n",
    "\n",
    "def visualize_tensor(tensor):\n",
    "    \"\"\"\n",
    "    visualize a tensor of shape (H, W, 1). Must be in range [-1, 1] or [0, 255].\n",
    "    \"\"\"\n",
    "    if tensor.dtype == torch.float32:\n",
    "        tensor = descale(tensor)\n",
    "    array = tensor.permute(1, 2, 0).contiguous().numpy()\n",
    "    plt.imshow(array, cmap='Greys_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_size = 12  # 64 // 5\n",
    "crop_h_start = (64 // 2) - (digit_size // 2)\n",
    "crop_h_end = crop_h_start + digit_size +1  # +1 is safety margin\n",
    "crop_w_start = 2\n",
    "crop_w_end = -2\n",
    "\n",
    "def split_digits(tensor, num_digits, digit_size=digit_size):\n",
    "    \"\"\"\n",
    "    splits a single image of `num_digits` digits to multiple images\n",
    "    \"\"\"\n",
    "    start_index = (5 - num_digits) * digit_size // 2\n",
    "    end_index = start_index + (num_digits * digit_size)\n",
    "    tensor = tensor[..., start_index: end_index]\n",
    "    return torch.stack(torch.chunk(tensor, num_digits, dim=2))\n",
    "\n",
    "# remove padding and rescale\n",
    "X_train_processed = scale(X_train[..., crop_h_start: crop_h_end, crop_w_start: crop_w_end])\n",
    "y_train_processed = y_train.to(torch.int64)\n",
    "X_test_processed = scale(X_test[..., crop_h_start: crop_h_end, crop_w_start: crop_w_end])\n",
    "\n",
    "\n",
    "# TODO: Create validation set randomly (with a fixed random seed) and val_loader\n",
    "num_digits_train_dataset = None\n",
    "num_digits_train_loader = None\n",
    "num_digits_val_dataset = None\n",
    "num_digits_val_loader = None\n",
    "\"\"\"\n",
    "dataset = TensorDataset(X_train_processed, y_train_processed)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\"\"\"\n",
    "\n",
    "# TODO: create a new dataset and loader from the previous dataset by splitting all digits and stacking them all together\n",
    "# such that the new dataset is of shape (number of total digits x 1 x 28 x 28)\n",
    "digits_train_dataset = None\n",
    "digits_train_loader = None\n",
    "digits_val_dataset = None\n",
    "digits_val_loader = None\n",
    "\n",
    "test_loader = DataLoader(X_test_processed, batch_size=4)\n",
    "\n",
    "# Class to represent our dataset and be used with dataloader\n",
    "class CustomMNISTDataset(TensorDataset):\n",
    "    def __init__(self):\n",
    "        self.len = len(X_train_processed)\n",
    "        self.x_data = X_train_processed\n",
    "        self.y_data = y_train_processed\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "dataset = CustomMNISTDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Raw X_train')\n",
    "visualize_tensor(X_train[102])\n",
    "print('------\\n')\n",
    "print('Preprocessed X_train')\n",
    "x_sample, y_sample = next(iter(train_loader))\n",
    "print(f'Label: {y_sample[0]}')\n",
    "visualize_tensor(x_sample[0])\n",
    "print('Splitted')\n",
    "num_digits = torch.sum(y_sample[0] != 10).item()\n",
    "splitted_digits = split_digits(x_sample[0], num_digits)\n",
    "for digit in splitted_digits:\n",
    "    visualize_tensor(digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convolutional neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: STEP 1\n",
    "# Train a NN to predict num_digits in each sample. I.e. input would be X from num_digits_train_dataset, output would be\n",
    "# torch.sum(y[0] != 10).item()\n",
    "# This model should have accuracy very close to 100% (>99% on validation)\n",
    "# IF IT DOESN'T, MIGHT NEED DATA AUGMENTATION HERE\n",
    "# Track validation accuracy during training to avoid overfitting (stop training when\n",
    "# validation accuracy starts to decrease)\n",
    "# FANCY TIPS: Use batchnorm layers, lookup VGG/RESNET architecture\n",
    "\n",
    "# TODO: STEP 2\n",
    "# Train a NN to predict each 12x12 digit in digits_train_dataset\n",
    "# Track validation accuracy during training to avoid overfitting (stop training when\n",
    "# validation accuracy starts to decrease)\n",
    "# FANCY TIPS: Use batchnorm layers, lookup VGG/RESNET architecture\n",
    "\n",
    "# TODO: STEP 3\n",
    "# Make num_digits prediction for X_test based on model in STEP1, then use `split_digits()` function \n",
    "# with the predicted number of digits to split the digits, then make a prediction for each digit\n",
    "# separately by using the model trained in STEP 2. Convert the predictions from e.g. 2, 3\n",
    "# into the required format e.g. [2, 3, 10, 10, 10]\n",
    "\n",
    "# TODO: STEP 4\n",
    "# Convert predictions into whatever Excel format is required for uploading to Kaggle\n",
    "\n",
    "# TODO: EXTRA IMPROVEMENTS\n",
    "# - Data Augmentation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # Ensure model weights initialized with same random numbers\n",
    "\n",
    "# # model 0\n",
    "# num_filters = 8\n",
    "# filter_size = 5\n",
    "# pool_size = 4\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Conv2d(in_channels=1,\n",
    "#                     out_channels=num_filters,\n",
    "#                     kernel_size=filter_size,\n",
    "#                     padding=filter_size // 2),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "#     torch.nn.Flatten(),\n",
    "#     torch.nn.Linear(num_filters * 64**2 // pool_size**2, 10),\n",
    "# )\n",
    "\n",
    "\n",
    "# model 1\n",
    "num_filters = [6, 16]\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "mlp_size = [16*14*14, 120, 84, 5 * 11]\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=1,\n",
    "                    out_channels=num_filters[0],\n",
    "                    kernel_size=filter_size,\n",
    "                    padding=filter_size // 2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "    # torch.nn.Conv2d(in_channels=num_filters[0],\n",
    "    #                 out_channels=num_filters[1],\n",
    "    #                 kernel_size=filter_size,\n",
    "    #                 padding=filter_size // 2),\n",
    "    # torch.nn.ReLU(),\n",
    "    # torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(1080, mlp_size[1]),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(mlp_size[1], mlp_size[2]),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(mlp_size[2], mlp_size[3]),\n",
    ")\n",
    "\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss function and Optimizer\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cross entropy is pretty good for classification problems\n",
    "# discuss pros of cross entropy in the report\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# might need to play a bit with learning rate and momentum if needed\n",
    "# SGD is computationally efficient\n",
    "# GD is terrible, uses too much memory\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters())  # converges faster and less sensitive to lr, generalizes (typiaclly) worse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training CNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loop over dataset multiple times\n",
    "# model.train()\n",
    "for epoch in range(4):\n",
    "    running_loss = 0 # counter\n",
    "    # get inputs, data is a list of [inputs, labels]\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss = 0\n",
    "        for j in range(5):\n",
    "            loss += criterion(outputs[:, (j * 11):((j + 1) * 11)], labels[:, j])\n",
    "\n",
    "        loss.backward() # computing gradients wrt model's weights\n",
    "        optimizer.step() # update using learning rate\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # print every 2000 minibatches\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss/2000))\n",
    "            running_loss = 0\n",
    "\n",
    "print('Done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_input = next(iter(test_loader))\n",
    "test_output = model(test_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def output_to_label(output):\n",
    "    labels = [torch.argmax(output[:, (j * 11):((j + 1) * 11)], dim=1) for j in range(5)]\n",
    "    return torch.stack(labels)\n",
    "test_pred_labels = output_to_label(test_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_pred_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # Ensure model weights initialized with same random numbers\n",
    "\n",
    "# # model 0\n",
    "# num_filters = 8\n",
    "# filter_size = 5\n",
    "# pool_size = 4\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Conv2d(in_channels=1,\n",
    "#                     out_channels=num_filters,\n",
    "#                     kernel_size=filter_size,\n",
    "#                     padding=filter_size // 2),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "#     torch.nn.Flatten(),\n",
    "#     torch.nn.Linear(num_filters * 64**2 // pool_size**2, 10),\n",
    "# )\n",
    "\n",
    "\n",
    "# model 1\n",
    "num_filters = [6, 16]\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "mlp_size = [4096, 120, 84, 5 * 11]\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=1,\n",
    "                    out_channels=num_filters[0],\n",
    "                    kernel_size=filter_size,\n",
    "                    padding=filter_size // 2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "    torch.nn.Conv2d(in_channels=num_filters[0],\n",
    "                    out_channels=num_filters[1],\n",
    "                    kernel_size=filter_size,\n",
    "                    padding=filter_size // 2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=pool_size, stride=pool_size),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(720, mlp_size[1]),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(mlp_size[1], mlp_size[2]),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(mlp_size[2], mlp_size[3]),\n",
    ")\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loss function and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy is pretty good for classification problems\n",
    "# discuss pros of cross entropy in the report\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# might need to play a bit with learning rate and momentum if needed\n",
    "# SGD is computationally efficient\n",
    "# GD is terrible, uses too much memory\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters())  # converges faster and less sensitive to lr, generalizes (typiaclly) worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loop over dataset multiple times\n",
    "# model.train()\n",
    "for epoch in range(4):\n",
    "    running_loss = 0 # counter\n",
    "    # get inputs, data is a list of [inputs, labels]\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        # loss = criterion(outputs, labels)\n",
    "        loss = 0\n",
    "        for j in range(5):\n",
    "            loss += criterion(outputs[:, (j * 11):((j + 1) * 11)], labels[:, j])\n",
    "\n",
    "        loss.backward() # computing gradients wrt model's weights\n",
    "        optimizer.step() # update using learning rate\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # print every 2000 minibatches\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss/2000))\n",
    "            running_loss = 0\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_input = next(iter(test_loader))\n",
    "test_output = model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_label(output):\n",
    "    labels = [torch.argmax(output[:, (j * 11):((j + 1) * 11)], dim=1) for j in range(5)]\n",
    "    return torch.stack(labels)\n",
    "test_pred_labels = output_to_label(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs, labels\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}