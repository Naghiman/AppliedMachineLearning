{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Import statements and data extraction\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\\ Import libraries and dataset\n"
    }
   },
   "outputs": [],
   "source": [
    "import h5py as h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "filename = 'MNIST_synthetic.h5'\n",
    "f = h5py.File(filename, 'r')\n",
    "\n",
    "train_dataset = f['train_dataset'][...]\n",
    "train_labels = f['train_labels'][...]\n",
    "test_dataset = f['test_dataset'][...]\n",
    "f.close()\n",
    "\n",
    "# if you have less cores or not a lot of RAM change the num_workers value\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=6)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset2 = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader2 = torch.utils.data.DataLoader(trainset2, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre processing data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# TODO: pre process data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convolutional neural network\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NOT DONE\n",
    "\n",
    "# For building CNN layers, we have the following hyperparameters :\n",
    "# Kernel size : Sets the size of the filter inside the layer. Kernel = Filter\n",
    "# Out channels : Sets the number of filters. One filter produces one output channel.\n",
    "# Out features : Sets the size of the output tensor\n",
    "# Inside a convolutional layer, the input channels are paired with a convolutional filter\n",
    "# to perform the convolutional operation. The filter convolves the input channel and\n",
    "# the result of this operation is an output channel.\n",
    "\n",
    "# Each layer in NN has 2 primary components : transformation and collection of weights\n",
    "# transformation represented as code\n",
    "# collection of weights represented as data\n",
    "# pytorch layers are defined by classes\n",
    "# Every pytorch nn.Module has a forward method that needs to be implemented\n",
    "# When we're building layers, we need to implement forward. Forward is the transformation\n",
    "# pytorch neural network\n",
    "\n",
    "# CNN requires linear and convolution layers\n",
    "# We should have different configs to determine which config is better\n",
    "# e.g. 1 config with more conv layers, or different kernel sizes, etc\n",
    "# Convolutional layers reduce memory usage and compute faster.\n",
    "# Convolutional layers work better than fully connected ones because\n",
    "# they are lighter and more efficient at learning spatial features.\n",
    "# https://www.sicara.ai/blog/2019-10-31-convolutional-layer-convolution-kernel\n",
    "# https://icecreamlabs.com/2018/08/19/3x3-convolution-filters%E2%80%8A-%E2%80%8Aa-popular-choice/\n",
    "\n",
    "# we will use 3x3 for kernel size, also its faster to train than 5x5 kernel size\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # in_channels 1 because our data is grayscale? it's 56000x64x64x1\n",
    "        # our input image is 1x64x64\n",
    "        # kernel_size being 3 means that we're doing 3x3 square convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3) # image dimension is 3xNxN\n",
    "        # output has dimensions 6x62x62\n",
    "        # with max_pool it's 16*31x31\n",
    "        # second convolutional layer, i.e. the output of the first layer will be the input of this layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3)\n",
    "        # output has dimensions 16 * 29 * 29\n",
    "        # with max pool it's 16 * 14 * 14\n",
    "\n",
    "        # now that we've done some convolution, we want to flatten the output\n",
    "        # and use it as input for linear layers\n",
    "\n",
    "        # after we run our image through the first 2 convolutional layers\n",
    "        self.fc1 = nn.Linear(in_features=16 * 14 * 14 , out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "\n",
    "        # output layer\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=11) # 11 because we have 11 labels\n",
    "\n",
    "    # Takes in tensor t and transform it using layer, new tensor is then returned\n",
    "    def forward(self, x):\n",
    "        # Picking our activation function for our cnn :\n",
    "        # We decided to use ReLu instead of a sigmoid function.\n",
    "        # Training a neural network is computationally expensive, ReLu is less expensive to compute\n",
    "        # because it does not need to compute exponential operations like Sigmoid.\n",
    "        # From stackoverflow:\n",
    "        # The biggest advantage of ReLu is indeed non-saturation of its gradient,\n",
    "        # which greatly accelerates the convergence of stochastic gradient descent compared to the sigmoid / tanh functions\n",
    "        # http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf\n",
    "\n",
    "        # max pooling over a (2,2) window (window of 4 elements)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "        # before we can pass it into the fully connected layer, i.e. fc1,f2,fc3\n",
    "        # we need to flatten it down to a single tensor\n",
    "        x = x.view(-1, self.num_flat_features(x)) # output shape is (1, 16 * 14 * 14)\n",
    "\n",
    "        # We run it through our linear layers now\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        \"\"\"\n",
    "        Helper function\n",
    "        :param x:\n",
    "        :return: the number of flat features that we have\n",
    "        \"\"\"\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%% Instantiate our CNN\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=3136, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss function and Optimizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy is pretty good for classification problems\n",
    "# discuss pros of cross entropy in the report\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# might need to play a bit with learning rate and momentum if needed\n",
    "# SGD is computationally efficient\n",
    "# GD is terrible, uses too much memory\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training CNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]]],\n",
      "\n",
      "\n",
      "        [[[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]]],\n",
      "\n",
      "\n",
      "        [[[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]]],\n",
      "\n",
      "\n",
      "        [[[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]],\n",
      "\n",
      "         [[0],\n",
      "          [0],\n",
      "          [0],\n",
      "          ...,\n",
      "          [0],\n",
      "          [0],\n",
      "          [0]]]], dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-40-f4905d314fc3>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrainloader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m         \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[1;31m# zero the parameter gradients\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# loop over dataset multiple times\n",
    "for epoch in range(2):\n",
    "    running_loss = 0 # counter\n",
    "    # get inputs, data is a list of [inputs, labels]\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        print(data)\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = Network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() # computing gradients wrt model's weights\n",
    "        optimizer.step() # update using learning rate\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # print every 2000 minibatches\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss/2000))\n",
    "            running_loss = 0\n",
    "\n",
    "print('Done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}